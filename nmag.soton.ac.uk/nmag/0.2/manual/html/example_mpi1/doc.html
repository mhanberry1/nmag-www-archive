

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>2.27. Example: Parallel execution (MPI) &mdash; NMAG User Manual v0.2.1 documentation</title>
    <link rel="stylesheet" href="../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.2.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <link rel="top" title="NMAG User Manual v0.2.1 documentation" href="../index.html" />
    <link rel="up" title="2. Guided Tour" href="../guided_tour.html" />
    <link rel="next" title="2.29. More than one magnetic material, exchange coupled" href="../example_excoupling/doc.html" />
    <link rel="prev" title="2.26. Example: Timestepper tolerances" href="../example_tolerances/doc.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../example_excoupling/doc.html" title="2.29. More than one magnetic material, exchange coupled"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="../example_tolerances/doc.html" title="2.26. Example: Timestepper tolerances"
             accesskey="P">previous</a> |</li>
        <li><a href="../manual.html">NMAG User Manual v0.2.1 documentation</a> &raquo;</li>
          <li><a href="../guided_tour.html" accesskey="U">2. Guided Tour</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
          <div class="body">
            
  <div class="section" id="example-parallel-execution-mpi">
<span id="id1"></span><h1>2.27. Example: Parallel execution (MPI)<a class="headerlink" href="doc.html#example-parallel-execution-mpi" title="Permalink to this headline">¶</a></h1>
<p>Nmag&#8216;s numerical core (which is part of the nsim multi-physics
library) has been designed to carry out numerical computation on
several CPUs simultaneously. The protocol that we are using for this
is the wide spread Message Passing Interface (MPI). There are a number
of MPI implementations; the best known ones are probably MPICH1,
MPICH2 and LAM-MPI. Currently, we support <a class="reference external" href="http://www-unix.mcs.anl.gov/mpi/mpich1/">MPICH1</a> and <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpich2">MPICH2</a>.</p>
<p>Which mpi version to use? Whether you want to use mpich1 or mpich2
will depend on your installation: currently, the installation from
source provides mpich2 (which is also used in the virtual machines)
whereas the Debian package relies on mpich1 (no Debian package is
provided after release 0.1-6163).</p>
<div class="section" id="using-mpich2">
<span id="id2"></span><h2>2.27.1. Using mpich2<a class="headerlink" href="doc.html#using-mpich2" title="Permalink to this headline">¶</a></h2>
<p>Before
the actual simulation is started, a <cite>multi-purpose daemon</cite> must be
started when using MPICH2.</p>
<hr class="docutils" />
<p><strong>The ``.mpd.conf`` file</strong></p>
<p>MPICH2 will look for a configuration file with name <tt class="docutils literal"><span class="pre">.mpd.conf</span></tt> in
the user&#8217;s home directory. If this is missing, an attempt to start
the multi-purpose daemon, will result in an error message like
this:</p>
<div class="highlight-python"><pre>$&gt; mpd
configuration file /Users/fangohr/.mpd.conf not found
A file named .mpd.conf file must be present in the user's home
directory (/etc/mpd.conf if root) with read and write access
only for the user, and must contain at least a line with:
MPD_SECRETWORD=&lt;secretword&gt;
One way to safely create this file is to do the following:
  cd $HOME
  touch .mpd.conf
  chmod 600 .mpd.conf
and then use an editor to insert a line like
  MPD_SECRETWORD=mr45-j9z
into the file.  (Of course use some other secret word than mr45-j9z.)</pre>
</div>
<p>If you don&#8217;t have this file in your home directory, just follow the
instructions above to create it with some secret word of your choice
(Note that the above example is from a Mac OS X system: on Linux the
home directory is usually under <tt class="docutils literal"><span class="pre">/home/USERNAME</span></tt> rather than
<tt class="docutils literal"><span class="pre">/Users/USERNAME</span></tt> as shown here.)</p>
<hr class="docutils" />
<p>Let&#8217;s assume we have a multi-core machine with more than one
CPU. This makes the mpi setup slightly easier, and is also likely to
be more efficient than running a job across the network between
difference machines.</p>
<p>First, we need to start the multi-purpose daemon:</p>
<div class="highlight-python"><pre>$&gt; mpd &amp;</pre>
</div>
<p>It will look  for the file <tt class="docutils literal"><span class="pre">~/.mpd.conf</span></tt> as described above. If found, it will start silently. Otherwise it will complain.</p>
<div class="section" id="testing-that-nsim-executes-in-parallel">
<span id="id3"></span><h3>2.27.1.1. Testing that nsim executes in parallel<a class="headerlink" href="doc.html#testing-that-nsim-executes-in-parallel" title="Permalink to this headline">¶</a></h3>
<p>First, let&#8217;s make sure that <tt class="docutils literal"><span class="pre">nsim</span></tt> is in the search path. The command <tt class="docutils literal"><span class="pre">which</span> <span class="pre">nsim</span></tt> will return the location of the executable if it can be found in the search path. For example:</p>
<div class="highlight-python"><pre>$&gt; which nsim
/home/fangohr/new/nmag-0.1/bin/nsim</pre>
</div>
<p>To execute nsim using two processes, we can use the command:</p>
<div class="highlight-python"><pre>$&gt; mpiexec -n 2 nsim</pre>
</div>
<p>There are two useful commands to check whether nsim is aware of the intended MPI setup. The fist one is <tt class="docutils literal"><span class="pre">ocaml.mpi_status()</span></tt> which provides the total number of processes in the MPI set-up:</p>
<div class="highlight-python"><pre>$&gt; mpiexec -n 2 nsim
&gt;&gt;&gt; ocaml.mpi_status()
MPI-status: There are 2 nodes (this is the master, rank=0)
&gt;&gt;&gt;</pre>
</div>
<p>The other command is <tt class="docutils literal"><span class="pre">ocaml.mpi_hello()</span></tt> and prints a short &#8216;hello&#8217; from all processes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">ocaml</span><span class="o">.</span><span class="n">mpi_hello</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">[</span><span class="n">Node</span>   <span class="mi">0</span><span class="o">/</span><span class="mi">2</span><span class="p">]</span> <span class="n">Hello</span> <span class="kn">from</span> <span class="nn">beta.kk.soton.ac.uk</span>
<span class="go">[Node   1/2] Hello from beta.kk.soton.ac.uk</span>
</pre></div>
</div>
<p>For comparison, let&#8217;s look at the output of these commands if we start <tt class="docutils literal"><span class="pre">nsim</span></tt> <em>without</em> MPI, in which case only one MPI node is reported:</p>
<div class="highlight-python"><pre>$&gt; nsim
&gt;&gt;&gt; ocaml.mpi_status()
MPI-status: There are 1 nodes (this is the master, rank=0)
&gt;&gt;&gt; ocaml.mpi_hello()
[Node   0/1] Hello from beta.kk.soton.ac.uk</pre>
</div>
<p>Assuming this all works, we can now start the actual simulation. To
use two CPUs on the local machine to run the <tt class="docutils literal"><span class="pre">bar30_30_100.py</span></tt>
program, we can use:</p>
<div class="highlight-python"><pre>$&gt; mpiexec -n 2 nsim bar30_30_100.py</pre>
</div>
<p>To run the program again, using 4 CPUs on the local machine:</p>
<div class="highlight-python"><pre>$&gt; mpiexec -n 4 nsim bar30_30_100.py</pre>
</div>
<p>Note that mpich2 (and mpich1) will spawn more processes than there are
CPUs if necessary. I.e. if you are working on some Intel Dual Core
processor (with 2 CPUs and one core each) but request to run your
program with 4 (via the <tt class="docutils literal"><span class="pre">-n</span> <span class="pre">4</span></tt> switch given to <tt class="docutils literal"><span class="pre">mpiexec</span></tt>), than
you will have 4 processes running on the 2 CPUs.</p>
<p>If you want to stop the <tt class="docutils literal"><span class="pre">mpd</span></tt> daemon, you can use:</p>
<div class="highlight-python"><pre>$&gt; mpdallexit</pre>
</div>
<p>For diagnostic purposes, the <tt class="docutils literal"><span class="pre">mpdtrace</span></tt> command can be use to track
whether a multipurpose daemon is running (and which machines are part
of the <cite>mpi-ring</cite>).</p>
<p><strong>Advanced usage of mpich2</strong></p>
<p>To run a job across different machines, one needs to start the
multi-purpose daemons on the other machines with the <tt class="docutils literal"><span class="pre">mpdboot</span></tt>
command. This will search for a file (in the current directory) with
name <tt class="docutils literal"><span class="pre">mpd.hosts</span></tt> which should contain a list of hosts to participate
(very similar to the <tt class="docutils literal"><span class="pre">machinefile</span></tt> in MPICH1).</p>
<p>To trace which process is sending what messages to the standard out,
one can add the <tt class="docutils literal"><span class="pre">-l</span></tt> switch to the <tt class="docutils literal"><span class="pre">mpiexec</span></tt> command: then each
line of standard output will be preceded by the rank of the process
who has issued the message.</p>
<p>Please refer to the official <a class="reference external" href="http://www.mcs.anl.gov/research/projects/mpich2">MPICH2</a>  documentation for further details.</p>
</div>
</div>
<div class="section" id="using-mpich1">
<span id="id4"></span><h2>2.27.2. Using mpich1<a class="headerlink" href="doc.html#using-mpich1" title="Permalink to this headline">¶</a></h2>
<p>Note: Most users will use MPICH2 (if they have compiled Nmag from the tar-ball): see <a class="reference internal" href="doc.html#using-mpich2"><em>Using mpich2</em></a></p>
<p>Suppose we would like to run <a class="reference internal" href="../example2/doc.html#example-2"><em>Example 2: Computing the time development of a system</em></a> of the manual with 2
processors using MPICH1. We need to know the full path to the <tt class="docutils literal"><span class="pre">nsim</span></tt> executable. In
a <tt class="docutils literal"><span class="pre">bash</span></tt> environment (this is pretty much the standard on Linux and
Mac OS X nowadays), you can find the path using the <tt class="docutils literal"><span class="pre">which</span></tt>
command. On a system where nsim was installed from the Debian package:</p>
<div class="highlight-python"><pre>$&gt; which nsim
/usr/bin/nsim</pre>
</div>
<p>Let&#8217;s assume we have a multi-core machine with more than one
CPU. This makes the mpi setup slightly easier, and is also likely to
be more efficient than running a job across the network between
difference machines. In that case, we can run the example on 2 CPUs using:</p>
<div class="highlight-python"><pre>$&gt; mpirun -np 2 /usr/bin/nsim bar30_30_100.py</pre>
</div>
<p>where <tt class="docutils literal"><span class="pre">-np</span></tt> is the command line argument for the Number of Processors.</p>
<p>To check that the code is running on more than one CPU, one of the
first few log messages will display (in addition to the runid of the
simulation) the number of CPUs used:</p>
<div class="highlight-python"><pre>$&gt; mpirun -np 2 `which nsim` bar30_30_100.py

    nmag:2008-05-20 12:50:01,177   setup.py  269    INFO Runid (=name simulation) is 'bar30_30_100', using 2 CPUs</pre>
</div>
<p>To use 4 processors (if we have a quad core machine available), we would use:</p>
<div class="highlight-python"><pre>$&gt; mpirun -np 4 /usr/bin/nsim bar30_30_100.py</pre>
</div>
<p>Assuming that the <tt class="docutils literal"><span class="pre">nsim</span></tt> executable is in the path, and that we are
using a bash-shell, we could shortcut the step of finding the <tt class="docutils literal"><span class="pre">nsim</span></tt>
executable by writing:</p>
<div class="highlight-python"><pre>$&gt; mpirun -np 4 `which nsim` bar30_30_100.py</pre>
</div>
<p>To run the job across the network on different machines
simultaneously, we need to create a file with the names of the hosts
that should be used for the parallel execution of the program. If you
intend to use nmag on a cluster, your cluster administrator should
explain where to find this machine file.</p>
<p>To distribute a job across <tt class="docutils literal"><span class="pre">machine1.mydomain</span></tt>,
<tt class="docutils literal"><span class="pre">machine2.mydomain</span></tt>, and <tt class="docutils literal"><span class="pre">machine3.mydomain</span></tt> we need to create the
file <tt class="docutils literal"><span class="pre">machines.txt</span></tt> with content:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="n">machine1</span><span class="o">.</span><span class="n">mydomain</span>
<span class="n">machine2</span><span class="o">.</span><span class="n">mydomain</span>
<span class="n">machine3</span><span class="o">.</span><span class="n">mydomain</span>
</pre></div>
</div>
<p>We then need to pass the name of this file to the <tt class="docutils literal"><span class="pre">mpirun</span></tt> command
to run a (mpi-enabled) executable with mpich:</p>
<div class="highlight-python"><pre>mpirun -machinefile machines.txt -np 3 /usr/bin/nsim bar30_30_100.py</pre>
</div>
<p>For further details, please refer to the <a class="reference external" href="http://www-unix.mcs.anl.gov/mpi/mpich1/">MPICH1</a> documentation.</p>
</div>
<div class="section" id="visualising-the-partition-of-the-mesh">
<span id="id5"></span><h2>2.27.3. Visualising the partition of the mesh<a class="headerlink" href="doc.html#visualising-the-partition-of-the-mesh" title="Permalink to this headline">¶</a></h2>
<p>We use Metis to partition the mesh. Partitioning means to allocate
certain mesh nodes to certain CPUs. Generally, it is good if nodes
that are spatially close to each other are assigned to the same CPU.</p>
<p>Here we demonstrate how the chosen partition can be visualised. As an
example, we use the <a class="reference internal" href="../example1/doc.html#example-1"><em>Example: demag field in uniformly magnetised
sphere</em></a>. We are <a class="reference internal" href="doc.html#using-mpich2"><em>Using mpich2</em></a>:</p>
<div class="highlight-python"><pre>$&gt; mpd &amp;
$&gt; mpiexec -l -n 3 nsim sphere1.py</pre>
</div>
<p>The program starts, and prints the chose partition to stdout:</p>
<div class="highlight-python"><pre> nfem.ocaml:2008-05-28 15:11:07,757    INFO Calling ParMETIS to partition the me
sh among 3 processors
 nfem.ocaml:2008-05-28 15:11:07,765    INFO Processor 0: 177 nodes
 nfem.ocaml:2008-05-28 15:11:07,765    INFO Processor 1: 185 nodes
 nfem.ocaml:2008-05-28 15:11:07,766    INFO Processor 2: 178 nodes</pre>
</div>
<p>If you can&#8217;t find the information on the screen (=stdout), then have a
look in <tt class="docutils literal"><span class="pre">sphere1_log.log</span></tt> which contains a copy of the log messages
that have been printed to stdout.</p>
<p>If we save any fields spatially resolved (as with the
<tt class="docutils literal"><span class="pre">sim.save_data(fields='all')</span></tt> command), then nmag will create a file
with name (in this case) <tt class="docutils literal"><span class="pre">sphere1_dat.h5</span></tt>. In addition to the field
data that is saved, it also stores the finite element mesh <em>in the
order that was used when the file was created</em>. In this example, this
is the mesh ordered according to the output from the ParMETIS
package. The first 177 nodes of the mesh in this order are assigned to
CPU0, the next 185 are assigned to CPU1, and the next 178 are assigned to
CPU2.</p>
<p>We can visualise this partition using the <a class="reference internal" href="../executables.html#nmeshpp"><em>nmeshpp</em></a> command (which we
apply here to the mesh that is saved in the <tt class="docutils literal"><span class="pre">sphere1_dat.h5</span></tt> file):</p>
<div class="highlight-python"><pre>$&gt; nmeshpp --partitioning=[177,185,178] sphere1_dat.h5 partitioning.vtk</pre>
</div>
<p>The new file <tt class="docutils literal"><span class="pre">partitioning.vtk</span></tt> contains only one field on the mesh, and this has assigned to each mesh node the id of the associated CPU. We can visualise this, for example, using:</p>
<div class="highlight-python"><pre>$&gt; mayavi -d partitioning.vtk -m SurfaceMap</pre>
</div>
<img alt="../_images/sphere3partitions.png" class="align-center" src="../_images/sphere3partitions.png" style="width: 800px; height: 664px;" />
<p>The figure shows that the sphere has been divided into three areas
which carry values 0, 1 and 2 (corresponding to the MPI CPU rank which
goes from 0 to 2 for 3 CPUs). Actually, in this plot we can only see
the surface nodes (but the volume nodes have been partitioned
accordingly).</p>
<p>The process described here is a bit cumbersome to visualise the
partition. This could in principle be streamlined (so that we save the
partition data into the <tt class="docutils literal"><span class="pre">_dat.h5</span></tt> data file and can generate the
visualisation without further manual intervention). However, we expect
that this is not a show stopper and will dedicate our time to more
pressing issues. (User feedback and suggestions for improvements are
of course always welcome.)</p>
</div>
<div class="section" id="performance">
<span id="id6"></span><h2>2.27.4. Performance<a class="headerlink" href="doc.html#performance" title="Permalink to this headline">¶</a></h2>
<p>Here is some data we have obtained on an IBM x440 system (with eight
1.9Ghz Intel Xeon processors). We use a test simulation (located in
<tt class="docutils literal"><span class="pre">tests/devtests/nmag/hyst/hyst.par</span></tt>) which computes a hysteresis
loop for a fairly small system (4114 mesh nodes, 1522 surface nodes,
BEM size 18MB). We use overdamped time integration to determine the
meta-stable states.</p>
<p>Both the setup and the time required to write data will not become
significantly faster when run on more than one CPU. We provide:</p>
<blockquote>
<div><p><strong>total time</strong>: this includes setup time, time for the main simulation loop and time for writing data (measured in seconds)</p>
<p><strong>total speedup</strong>: The speed up for the total execution time (i.e. ratio of execution time on one CPU to execution time on n CPUs).</p>
<p><strong>sim time</strong>: this is the time spend in the main simulation loop (and this is where expect a speed up)</p>
<p><strong>sim speedup</strong>: the speedup of the main simulation loop</p>
</div></blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="11%" />
<col width="21%" />
<col width="28%" />
<col width="17%" />
<col width="23%" />
</colgroup>
<tbody valign="top">
<tr><td>CPUs</td>
<td>total time</td>
<td>total speedup</td>
<td>sim time</td>
<td>sim speedup</td>
</tr>
<tr><td>1</td>
<td>4165</td>
<td>1.00</td>
<td>3939</td>
<td>1.00</td>
</tr>
<tr><td>2</td>
<td>2249</td>
<td>1.85</td>
<td>2042</td>
<td>1.93</td>
</tr>
<tr><td>3</td>
<td>1867</td>
<td>2.23</td>
<td>1659</td>
<td>2.37</td>
</tr>
<tr><td>4</td>
<td>1605</td>
<td>2.60</td>
<td>1393</td>
<td>2.83</td>
</tr>
</tbody>
</table>
<p>The numbers shown here have been obtained using mpich2 (and using the
<tt class="docutils literal"><span class="pre">ssm</span></tt> device instead of the default <tt class="docutils literal"><span class="pre">sock</span></tt> device: this is
available on Linux and resulted in a 5% reduction of execution time).</p>
<p>Generally, the (network) communication that is required between the
nodes will slow down the communication. The smaller the system, the
more communication has to happen between the nodes (relative to the
amount of time spent on actual calculation). Thus, one expects a
better speed up for larger systems. The performance of the network is
also crucial: generally, we expect the best speed up on very fast
networks and shared memory systems (i.e. multi-CPU / multi-core
architectures). We further expect the speed-up to become worse (in
comparison to the ideal linear speed-up) with an increasing number of
processes.</p>
</div>
</div>
<div class="section" id="restarting-mpi-runs">
<span id="id7"></span><h1>2.28. Restarting MPI runs<a class="headerlink" href="doc.html#restarting-mpi-runs" title="Permalink to this headline">¶</a></h1>
<p>There is one situation that should be avoided when exploiting parallel
computation. Usually, a simulation (involving for example a hysteresis
loop), can be continued using the <tt class="docutils literal"><span class="pre">--restart</span></tt> switch. This is also
true for MPI runs.</p>
<p>However, the number of CPUs used <em>must not change</em> between the initial
and any subsequent runs. (The reason for this is that the <tt class="docutils literal"><span class="pre">_dat.h5</span></tt>
file needs to store the mesh as it has been reordered for <em>n</em> CPUs. If
we continue the run with another number of CPUs, the mesh data will
not be correct anymore which will lead to errors when extracting the
data from the <tt class="docutils literal"><span class="pre">_dat.h5</span></tt> file.)</p>
<p>Note also that there is currently no warning issued (in Nmag 0.1) if a user ventures
into such a simulation.</p>
</div>


          </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../example_excoupling/doc.html" title="2.29. More than one magnetic material, exchange coupled"
             >next</a> |</li>
        <li class="right" >
          <a href="../example_tolerances/doc.html" title="2.26. Example: Timestepper tolerances"
             >previous</a> |</li>
        <li><a href="../manual.html">NMAG User Manual v0.2.1 documentation</a> &raquo;</li>
          <li><a href="../guided_tour.html" >2. Guided Tour</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2011, Hans Fangohr, Thomas Fischbacher, Matteo Franchin, Giuliano Bordignon, Jacek Generowicz, Andreas Knittel, Michael Walter, Maximilian Albert.
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.0.7.
    </div>
  </body>
</html>